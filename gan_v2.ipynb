{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator output shape torch.Size([1, 1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "\n",
    "\"\"\"\n",
    "Implementation based on original paper NeurIPS 2016\n",
    "https://papers.nips.cc/paper/6096-learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, dim=64, out_conv_channels=512):\n",
    "        super(Discriminator, self).__init__()\n",
    "        conv1_channels = int(out_conv_channels / 8)\n",
    "        conv2_channels = int(out_conv_channels / 4)\n",
    "        conv3_channels = int(out_conv_channels / 2)\n",
    "        self.out_conv_channels = out_conv_channels\n",
    "        self.out_dim = int(dim / 16)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=in_channels, out_channels=conv1_channels, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv1_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv2_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv3_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=conv3_channels, out_channels=out_conv_channels, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(out_conv_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(out_conv_channels * self.out_dim * self.out_dim * self.out_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # Flatten and apply linear + sigmoid\n",
    "        x = x.view(-1, self.out_conv_channels * self.out_dim * self.out_dim * self.out_dim)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, in_channels=512, out_dim=64, out_channels=1, noise_dim=200, activation=\"sigmoid\"):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = int(out_dim / 16)\n",
    "        conv1_out_channels = int(self.in_channels / 2.0)\n",
    "        conv2_out_channels = int(conv1_out_channels / 2)\n",
    "        conv3_out_channels = int(conv2_out_channels / 2)\n",
    "\n",
    "        self.linear = torch.nn.Linear(noise_dim, in_channels * self.in_dim * self.in_dim * self.in_dim)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(\n",
    "                in_channels=in_channels, out_channels=conv1_out_channels, kernel_size=(4, 4, 4),\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv1_out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(\n",
    "                in_channels=conv1_out_channels, out_channels=conv2_out_channels, kernel_size=(4, 4, 4),\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv2_out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(\n",
    "                in_channels=conv2_out_channels, out_channels=conv3_out_channels, kernel_size=(4, 4, 4),\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm3d(conv3_out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(\n",
    "                in_channels=conv3_out_channels, out_channels=out_channels, kernel_size=(4, 4, 4),\n",
    "                stride=2, padding=1, bias=False\n",
    "            )\n",
    "        )\n",
    "        if activation == \"sigmoid\":\n",
    "            self.out = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.out = torch.nn.Tanh()\n",
    "\n",
    "    def project(self, x):\n",
    "        \"\"\"\n",
    "        projects and reshapes latent vector to starting volume\n",
    "        :param x: latent vector\n",
    "        :return: starting volume\n",
    "        \"\"\"\n",
    "        return x.view(-1, self.in_channels, self.in_dim, self.in_dim, self.in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.project(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "def test_gan3d():\n",
    "    noise_dim = 200\n",
    "    in_channels = 512\n",
    "    dim = 64  # cube volume\n",
    "    model_generator = Generator(in_channels=512, out_dim=dim, out_channels=1, noise_dim=noise_dim)\n",
    "    noise = torch.rand(1, noise_dim)\n",
    "    generated_volume = model_generator(noise)\n",
    "    print(\"Generator output shape\", generated_volume.shape)\n",
    "    model_discriminator = Discriminator(in_channels=1, dim=dim, out_conv_channels=in_channels)\n",
    "    out = model_discriminator(generated_volume)\n",
    "\n",
    "\n",
    "test_gan3d()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "\n",
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    FAKE data\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_discriminator(discriminator, optimizer, real_data, fake_data, loss):\n",
    "    cuda = next(discriminator.parameters()).is_cuda\n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    target_real = ones_target(N)\n",
    "    if cuda:\n",
    "        target_real.cuda()\n",
    "\n",
    "    error_real = loss(prediction_real, target_real)\n",
    "    error_real.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    target_fake = zeros_target(N)\n",
    "    if cuda:\n",
    "        target_fake.cuda()\n",
    "    error_fake = loss(prediction_fake, target_fake)\n",
    "    error_fake.backward()\n",
    "\n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n",
    "\n",
    "\n",
    "def train_generator(discriminator, optimizer, fake_data, loss):\n",
    "    cuda = next(discriminator.parameters()).is_cuda\n",
    "    N = fake_data.size(0)  # Reset gradients\n",
    "    optimizer.zero_grad()  # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)  # Calculate error and backpropagate\n",
    "    target = ones_target(N)\n",
    "    if cuda:\n",
    "        target.cuda()\n",
    "\n",
    "    error = loss(prediction, target)\n",
    "    error.backward()  # Update weights with gradients\n",
    "    optimizer.step()  # Return error\n",
    "    return error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    # Code adapted from assignment text\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    train_voxel = torch.from_numpy(data[\"train_voxel\"]).float().unsqueeze(1) # Training 3D voxel samples\n",
    "    test_voxel = torch.from_numpy(data[\"test_voxel\"]).float().unsqueeze(1) # Test 3D voxel samples\n",
    "    train_labels = torch.from_numpy(data[\"train_labels\"]).long() # Training labels (integers from 0 to 9)\n",
    "    test_labels = torch.from_numpy(data[\"test_labels\"]).long() # Test labels (integers from 0 to 9)\n",
    "    class_map = data[\"class_map\"] # Dictionary mapping the labels to their class names.\n",
    "\n",
    "    return train_voxel, test_voxel, train_labels, test_labels, class_map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train_v, test_v, train_l, test_l, class_map = data_loader(\"data/modelnet10.npz\")\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_v)\n",
    "# Define a DataLoader\n",
    "batch_size = 50 # Somewhere between 10 and 100 as outlined in the assignment\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 21\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Your DataLoader should be defined as train_loader\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m n_batch, (real_batch, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     23\u001B[0m         N \u001B[38;5;241m=\u001B[39m real_batch\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     25\u001B[0m         \u001B[38;5;66;03m# Generate fake data\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize your models and optimizers here\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Your DataLoader should be defined as train_loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_batch, _) in enumerate(train_loader):\n",
    "\n",
    "        N = real_batch.size(0)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(N, 200)\n",
    "        fake_data = generator(noise).detach()\n",
    "\n",
    "        # Train Discriminator\n",
    "        real_data = real_batch\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(discriminator, optimizer_d, real_data, fake_data, loss)\n",
    "\n",
    "        # Generate fake data again but don't detach to compute gradients\n",
    "        noise = torch.randn(N, 200)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Train Generator\n",
    "        g_error = train_generator(discriminator, optimizer_g, fake_data, loss)\n",
    "\n",
    "        # Log batch error and other details if needed\n",
    "        print(f\"Epoch {epoch} Batch {n_batch} D Error: {d_error} G Error: {g_error}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# Hyperparameters\n",
    "lambda1 = 5.0  # Weight for the KL divergence loss\n",
    "lambda2 = 1e-4  # Weight for the reconstruction loss\n",
    "\n",
    "# Initialize the Binary Cross Entropy loss for the GAN loss\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "def gan_loss(logits_real, logits_fake):\n",
    "    \"\"\"\n",
    "    Compute the GAN loss.\n",
    "    \"\"\"\n",
    "    # Targets for real and fake data\n",
    "    label_real = torch.ones_like(logits_real)\n",
    "    label_fake = torch.zeros_like(logits_fake)\n",
    "\n",
    "    # Compute the loss for the discriminator\n",
    "    loss_real = bce_loss(logits_real, label_real)\n",
    "    loss_fake = bce_loss(logits_fake, label_fake)\n",
    "\n",
    "    # Compute the total GAN loss\n",
    "    loss_gan = loss_real + loss_fake\n",
    "\n",
    "    return loss_gan\n",
    "\n",
    "def kl_divergence_loss(mu, log_var):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss.\n",
    "    \"\"\"\n",
    "    loss_kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return loss_kl\n",
    "\n",
    "def reconstruction_loss(x, x_recon):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction loss.\n",
    "    \"\"\"\n",
    "    loss_recon = F.mse_loss(x_recon, x)\n",
    "    return loss_recon\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have the following tensors:\n",
    "# logits_real and logits_fake from the discriminator\n",
    "# mu and log_var from the encoder\n",
    "# x and x_recon for the real and reconstructed 3D shapes\n",
    "\n",
    "# Compute the individual losses\n",
    "loss_gan = gan_loss(logits_real, logits_fake)\n",
    "loss_kl = kl_divergence_loss(mu, log_var)\n",
    "loss_recon = reconstruction_loss(x, x_recon)\n",
    "\n",
    "# Compute the total loss\n",
    "total_loss = loss_gan + lambda1 * loss_kl + lambda2 * loss_recon"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Initialize the Binary Cross Entropy loss\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Hyperparameters for the custom losses\n",
    "lambda1 = 5.0  # Weight for the KL divergence loss\n",
    "lambda2 = 1e-4  # Weight for the reconstruction loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Number of epochs\n",
    "\n",
    "# Outer loop with tqdm\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "\n",
    "    # Inner loop with tqdm\n",
    "    for batch_idx, (real_voxels, _) in tqdm(enumerate(train_loader), desc='Batches', leave=False):\n",
    "\n",
    "        # Discriminator update\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Generate fake voxels\n",
    "        z = torch.randn(real_voxels.size(0), 200)\n",
    "        fake_voxels = generator(z)\n",
    "\n",
    "        # Compute discriminator logits for real and fake voxels\n",
    "        logits_real = discriminator(real_voxels)\n",
    "        logits_fake = discriminator(fake_voxels.detach())\n",
    "\n",
    "        # Compute the GAN loss for the discriminator\n",
    "        loss_d = gan_loss(logits_real, logits_fake)\n",
    "\n",
    "        # Backprop and optimize the discriminator\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Generator update\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # Compute discriminator logits for fake voxels\n",
    "        logits_fake = discriminator(fake_voxels)\n",
    "\n",
    "        # Compute the GAN loss for the generator\n",
    "        loss_g = -torch.mean(torch.log(logits_fake))\n",
    "\n",
    "        # Here you would typically compute the KL divergence and reconstruction losses\n",
    "        # For example:\n",
    "        # loss_kl = kl_divergence_loss(mu, log_var)\n",
    "        # loss_recon = reconstruction_loss(x, x_recon)\n",
    "\n",
    "        # Compute the total loss for the generator\n",
    "        # total_loss_g = loss_g + lambda1 * loss_kl + lambda2 * loss_recon\n",
    "\n",
    "        # Backprop and optimize the generator\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Print losses\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}], D Loss: {loss_d.item()}, G Loss: {loss_g.item()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
